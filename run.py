#!/usr/bin/env python3
"""
SciGenBench Unified Runner
ç»Ÿä¸€å¯åŠ¨è„šæœ¬ï¼šæ”¯æŒå›¾åƒç”Ÿæˆå’Œè¯„ä¼°
"""

import os
import sys
import argparse
import subprocess
import json
import pandas as pd
from pathlib import Path
from typing import List, Optional

# è·å–é¡¹ç›®æ ¹ç›®å½•
PROJECT_ROOT = Path(__file__).parent.absolute()
sys.path.insert(0, str(PROJECT_ROOT))

# ==================== é…ç½® ====================

# å¯ç”¨æ•°æ®é›†
DATASETS = ["scigen", "seephys"]

# å¯ç”¨æ¨¡å‹ï¼ˆæŒ‰æ•°æ®é›†åˆ†ç±»ï¼‰
AVAILABLE_MODELS = {
    "scigen": [
        "gemini-3-pro-imgcoder",
        "gemini-3-flash-imgcoder",
        "qwen3-imgcoder",
        "gpt-image1",
        "gpt-image1_5",
        "gpt-image1-mini",
        "nanobanana",
        "nanobananapro",
        "qwen-image-plus",
        "hunyuan",
        "flux2",
        "seedream",
    ],
    "seephys": [
        "gemini-3-pro-imgcoder",
        "gemini-3-flash-imgcoder",
        "qwen3-imgcoder",
        "gpt-image1",
        "gpt-image1_5",
        "nanobanana",
        "nanopro",  # seephys ä½¿ç”¨ nanopro è€Œä¸æ˜¯ nanobananapro
        "qwen-image-plus",
        "hunyuan",
        "flux2",
        "seedream",
    ]
}

# è¯„ä¼°æŒ‡æ ‡
EVAL_METRICS = {
    "judge": "LLM-as-Judge è¯„ä¼°ï¼ˆ5ç»´è¯„åˆ†ï¼‰",
    "quiz": "Inverse Quiz Validationï¼ˆé€†å‘éªŒè¯ï¼ŒåŒ…å« VQAï¼‰",
    "t2i": "Text-to-Image Metrics (PSNR, SSIM, CLIP, FID) - ä»…é€‚ç”¨äº seephys",
    "all": "æ‰€æœ‰è¯„ä¼°æŒ‡æ ‡",
}

# ==================== å·¥å…·å‡½æ•° ====================

def get_model_script_path(dataset: str, model: str) -> Path:
    """è·å–æ¨¡å‹è„šæœ¬è·¯å¾„"""
    # å¤„ç†æ¨¡å‹åç§°æ˜ å°„
    model_file_map = {
        "nanobananapro": "nanobananapro.py",
        "nanopro": "nanopro.py",
        "seedream": "seedream.py",
    }
    
    model_file = model_file_map.get(model, f"{model}.py")
    script_path = PROJECT_ROOT / "src" / "infer" / dataset / model_file
    
    if not script_path.exists():
        # å°è¯•å…¶ä»–å¯èƒ½çš„æ–‡ä»¶å
        alt_path = PROJECT_ROOT / "src" / "infer" / dataset / f"{model.replace('-', '_')}.py"
        if alt_path.exists():
            return alt_path
        
        # æä¾›æ›´è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯
        dataset_dir = PROJECT_ROOT / "src" / "infer" / dataset
        if not dataset_dir.exists():
            raise FileNotFoundError(f"Dataset directory not found: {dataset_dir}")
        
        # åˆ—å‡ºç›®å½•ä¸­çš„æ–‡ä»¶
        existing_files = list(dataset_dir.glob("*.py"))
        existing_names = [f.name for f in existing_files]
        raise FileNotFoundError(
            f"Model script not found: {script_path}\n"
            f"Available files in {dataset_dir}: {', '.join(sorted(existing_names))}"
        )
    
    return script_path

def get_eval_script_path(metric: str, dataset: str) -> Path:
    """è·å–è¯„ä¼°è„šæœ¬è·¯å¾„"""
    if metric == "judge":
        script_path = PROJECT_ROOT / "src" / "eval" / "llm_as_judge.py"
    elif metric == "quiz":
        if dataset == "scigen":
            script_path = PROJECT_ROOT / "src" / "eval" / "quiz.py"
        else:  # seephys
            script_path = PROJECT_ROOT / "src" / "eval" / "quiz_seephys.py"
    elif metric == "t2i":
        if dataset != "seephys":
            raise ValueError(f"T2I metric is only available for seephys dataset, got {dataset}")
        script_path = PROJECT_ROOT / "src" / "eval" / "t2i_metric.py"
    else:
        raise ValueError(f"Unknown metric: {metric}")
    
    if not script_path.exists():
        raise FileNotFoundError(f"Eval script not found: {script_path}")
    
    return script_path

def run_generation(dataset: str, model: str, verbose: bool = False) -> bool:
    """è¿è¡Œå›¾åƒç”Ÿæˆ"""
    print(f"\n{'='*60}")
    print(f"ğŸš€ Starting Image Generation")
    print(f"{'='*60}")
    print(f"Dataset: {dataset}")
    print(f"Model: {model}")
    print(f"{'='*60}\n")
    
    original_cwd = os.getcwd()
    try:
        script_path = get_model_script_path(dataset, model)
        
        # åˆ‡æ¢åˆ°è„šæœ¬æ‰€åœ¨ç›®å½•ï¼Œç¡®ä¿ç›¸å¯¹å¯¼å…¥æ­£å¸¸å·¥ä½œ
        script_dir = script_path.parent
        
        # è¿è¡Œè„šæœ¬
        cmd = [sys.executable, str(script_path.name)]
        if verbose:
            print(f"Running: {' '.join(cmd)}")
            print(f"Working directory: {script_dir}\n")
        
        # ä¸æ•è·è¾“å‡ºï¼Œè®©è¿›åº¦æ¡æ­£å¸¸æ˜¾ç¤º
        # tqdm è¿›åº¦æ¡éœ€è¦ç›´æ¥è¾“å‡ºåˆ°ç»ˆç«¯
        result = subprocess.run(
            cmd,
            cwd=str(script_dir),
            check=True,
            # ä¸æ•è·è¾“å‡ºï¼Œè®©è¿›åº¦æ¡å’Œå®æ—¶è¾“å‡ºæ­£å¸¸æ˜¾ç¤º
            stdout=None,
            stderr=None,
        )
        
        if result.returncode == 0:
            print(f"\nâœ… Generation completed successfully!")
            return True
        else:
            print(f"\nâŒ Generation failed with return code {result.returncode}")
            return False
            
    except FileNotFoundError as e:
        print(f"âŒ Error: {e}")
        return False
    except subprocess.CalledProcessError as e:
        print(f"âŒ Generation process failed with return code {e.returncode}")
        return False
    except Exception as e:
        print(f"âŒ Unexpected error: {e}")
        import traceback
        traceback.print_exc()
        return False
    finally:
        # æ¢å¤å·¥ä½œç›®å½•
        try:
            os.chdir(original_cwd)
        except:
            pass

def run_vqa_evaluation(dataset: str, model: str = None, verbose: bool = False) -> bool:
    """è¿è¡Œ VQA è¯„ä¼°ï¼ˆä½œä¸º quiz çš„ä¸€éƒ¨åˆ†ï¼‰"""
    original_cwd = os.getcwd()
    try:
        script_path = PROJECT_ROOT / "src" / "eval" / "ve_infer.py"
        script_dir = script_path.parent
        
        cmd = [sys.executable, str(script_path.name), "--dataset", dataset]
        if model:
            cmd.extend(["--model", model])
        
        if verbose:
            print(f"Running: {' '.join(cmd)}")
            print(f"Working directory: {script_dir}\n")
        
        # æ•è·è¾“å‡ºä»¥ä¾¿è°ƒè¯•
        result = subprocess.run(
            cmd,
            cwd=str(script_dir),
            check=False,  # ä¸æŠ›å‡ºå¼‚å¸¸ï¼Œè®©æˆ‘ä»¬è‡ªå·±å¤„ç†
            stdout=subprocess.PIPE,
            stderr=subprocess.STDOUT,
            text=True,
        )
        # æ‰“å°è¾“å‡ºä»¥ä¾¿è°ƒè¯•
        if result.stdout:
            print(result.stdout)
        if result.returncode == 0:
            return True
        else:
            print(f"\nâŒ VQA evaluation failed with return code {result.returncode}")
            return False
            
    except FileNotFoundError as e:
        print(f"âŒ Error: {e}")
        return False
    except Exception as e:
        print(f"âŒ Unexpected error: {e}")
        import traceback
        traceback.print_exc()
        return False
    finally:
        try:
            os.chdir(original_cwd)
        except:
            pass

def run_evaluation(dataset: str, metric: str, model: str = None, verbose: bool = False) -> bool:
    """è¿è¡Œè¯„ä¼°"""
    print(f"\n{'='*60}")
    print(f"ğŸ“Š Starting Evaluation")
    print(f"{'='*60}")
    print(f"Dataset: {dataset}")
    print(f"Metric: {metric} ({EVAL_METRICS[metric]})")
    print(f"{'='*60}\n")
    
    original_cwd = os.getcwd()
    try:
        script_path = get_eval_script_path(metric, dataset)
        
        # åˆ‡æ¢åˆ°è„šæœ¬æ‰€åœ¨ç›®å½•
        script_dir = script_path.parent
        
        # æ„å»ºå‘½ä»¤
        cmd = [sys.executable, str(script_path.name)]
        
        # å¯¹äº judgeï¼Œéœ€è¦ä¼ é€’ dataset å‚æ•°
        if metric == "judge":
            cmd.extend(["--dataset", dataset])
        
        # ä¼ é€’æ¨¡å‹å‚æ•°
        if model and metric in ["judge", "quiz", "t2i"]:
            cmd.extend(["--model", model])
        
        if verbose:
            print(f"Running: {' '.join(cmd)}")
            print(f"Working directory: {script_dir}\n")
        
        # ä¸æ•è·è¾“å‡ºï¼Œè®©è¿›åº¦æ¡æ­£å¸¸æ˜¾ç¤º
        # ä½†å¯¹äº t2iï¼Œæˆ‘ä»¬éœ€è¦æ•è·ä¸€äº›è¾“å‡ºä»¥ä¾¿è°ƒè¯•
        if metric == "t2i":
            result = subprocess.run(
                cmd,
                cwd=str(script_dir),
                check=False,  # ä¸æŠ›å‡ºå¼‚å¸¸ï¼Œè®©æˆ‘ä»¬è‡ªå·±å¤„ç†
                stdout=subprocess.PIPE,
                stderr=subprocess.STDOUT,
                text=True,
            )
            # æ‰“å°è¾“å‡ºä»¥ä¾¿è°ƒè¯•
            if result.stdout:
                print(result.stdout)
            if result.returncode == 0:
                print(f"\nâœ… Evaluation completed successfully!")
                return True
            else:
                print(f"\nâŒ Evaluation failed with return code {result.returncode}")
                if result.stdout:
                    # æ‰“å°æœ€åå‡ è¡Œé”™è¯¯ä¿¡æ¯
                    lines = result.stdout.strip().split('\n')
                    if len(lines) > 10:
                        print("Last 10 lines of output:")
                        for line in lines[-10:]:
                            print(f"  {line}")
                return False
        else:
            result = subprocess.run(
                cmd,
                cwd=str(script_dir),
                check=True,
                # ä¸æ•è·è¾“å‡ºï¼Œè®©è¿›åº¦æ¡å’Œå®æ—¶è¾“å‡ºæ­£å¸¸æ˜¾ç¤º
                stdout=None,
                stderr=None,
            )
            
            if result.returncode == 0:
                print(f"\nâœ… Evaluation completed successfully!")
                return True
            else:
                print(f"\nâŒ Evaluation failed with return code {result.returncode}")
                return False
            
    except FileNotFoundError as e:
        print(f"âŒ Error: {e}")
        return False
    except subprocess.CalledProcessError as e:
        print(f"âŒ Evaluation process failed with return code {e.returncode}")
        return False
    except Exception as e:
        print(f"âŒ Unexpected error: {e}")
        import traceback
        traceback.print_exc()
        return False
    finally:
        # æ¢å¤å·¥ä½œç›®å½•
        try:
            os.chdir(original_cwd)
        except:
            pass

def summarize_results(dataset: str, model: str, metrics: List[str]) -> None:
    """æ±‡æ€»è¯„ä¼°ç»“æœ"""
    print(f"\n{'='*60}")
    print(f"ğŸ“ˆ Results Summary")
    print(f"{'='*60}")
    print(f"Dataset: {dataset}")
    print(f"Model: {model}")
    print(f"{'='*60}\n")
    
    results_dir = PROJECT_ROOT / "results" / dataset
    
    # Judge ç»“æœ
    if "judge" in metrics:
        judge_dir = results_dir / "llm_as_judge"
        judge_file = judge_dir / f"{model}_quality_scores.csv"
        
        if judge_file.exists():
            try:
                df = pd.read_csv(judge_file)
                score_cols = [
                    "Correctness_Fidelity",
                    "Layout_Precision",
                    "Readability_Occlusion",
                    "Scientific_Plausibility",
                    "Expressiveness_Richness"
                ]
                
                valid_df = df.dropna(subset=score_cols)
                if not valid_df.empty:
                    print("ğŸ“Š LLM-as-Judge Scores (0-2 scale):")
                    print("-" * 60)
                    for col in score_cols:
                        mean_score = valid_df[col].mean()
                        print(f"  {col:<30}: {mean_score:.2f}")
                    print(f"  {'Total Samples':<30}: {len(valid_df)}")
                    print("-" * 60)
                else:
                    print("âš ï¸  No valid judge scores found")
            except Exception as e:
                print(f"âš ï¸  Error reading judge results: {e}")
        else:
            print("âš ï¸  Judge results file not found")
    
    # Quiz ç»“æœï¼ˆåˆå¹¶ quiz å’Œ vqa çš„ç»“æœï¼‰
    if "quiz" in metrics:
        quiz_dir = results_dir / "quiz"
        quiz_file = quiz_dir / f"{model}_detailed_evaluation.csv"
        vqa_dir = results_dir / "vqa"
        vqa_file = vqa_dir / f"{model}_eval_cot.csv"
        
        quiz_df = None
        vqa_df = None
        
        # è¯»å– quiz ç»“æœ
        if quiz_file.exists():
            try:
                quiz_df = pd.read_csv(quiz_file)
                quiz_df = quiz_df[quiz_df['quiz_idx'] >= 0].copy()
            except Exception as e:
                print(f"âš ï¸  Error reading quiz results: {e}")
        
        # è¯»å– vqa ç»“æœï¼ˆä»… scigen æ•°æ®é›†æœ‰ VQAï¼‰
        vqa_df = None
        if dataset == "scigen":
            if vqa_file.exists():
                try:
                    vqa_df = pd.read_csv(vqa_file)
                    # è¿‡æ»¤æ‰æœ‰é”™è¯¯çš„è®°å½•
                    if 'error_msg' in vqa_df.columns:
                        vqa_df = vqa_df[vqa_df['error_msg'].isna() | (vqa_df['error_msg'] == "")].copy()
                except Exception as e:
                    print(f"âš ï¸  Error reading VQA results: {e}")
        
        # åˆå¹¶ quiz å’Œ vqa ç»“æœï¼ˆä»… scigenï¼‰
        if dataset == "scigen" and quiz_df is not None and vqa_df is not None and not vqa_df.empty:
            # åˆå¹¶ä¸¤ä¸ª DataFrame
            # quiz ä½¿ç”¨ id, quiz_idx, is_correct
            # vqa ä½¿ç”¨ id, is_correctï¼Œéœ€è¦è½¬æ¢ä¸ºç›¸åŒæ ¼å¼
            quiz_combined = quiz_df[['id', 'is_correct']].copy()
            vqa_combined = vqa_df[['id', 'is_correct']].copy()
            
            # åˆå¹¶
            combined_df = pd.concat([quiz_combined, vqa_combined], ignore_index=True)
            
            total_questions = len(combined_df)
            total_correct = combined_df['is_correct'].sum()
            overall_acc = total_correct / total_questions if total_questions > 0 else 0
            
            # Image level perfect rateï¼ˆåŸºäº idï¼‰
            image_stats = combined_df.groupby('id')['is_correct'].mean()
            perfect_images = (image_stats == 1.0).sum()
            perfect_rate = perfect_images / len(image_stats) if len(image_stats) > 0 else 0
            
            print("\nğŸ“ Inverse Quiz Validation Results (Quiz + VQA combined):")
            print("-" * 60)
            print(f"  Quiz Questions    : {len(quiz_combined)}")
            print(f"  VQA Questions     : {len(vqa_combined)}")
            print(f"  Total Questions   : {total_questions}")
            print(f"  Question Accuracy : {overall_acc:.2%} ({total_correct}/{total_questions})")
            print(f"  Perfect Image Rate : {perfect_rate:.2%} ({perfect_images}/{len(image_stats)})")
            print("-" * 60)
        elif quiz_df is not None and not quiz_df.empty:
            # åªæœ‰ quiz ç»“æœï¼ˆseephys æˆ– scigen æ²¡æœ‰ vqa ç»“æœæ—¶ï¼‰
            total_questions = len(quiz_df)
            total_correct = quiz_df['is_correct'].sum()
            overall_acc = total_correct / total_questions if total_questions > 0 else 0
            
            image_stats = quiz_df.groupby('id')['is_correct'].mean()
            perfect_images = (image_stats == 1.0).sum()
            perfect_rate = perfect_images / len(image_stats) if len(image_stats) > 0 else 0
            
            if dataset == "scigen" and vqa_file.exists():
                print("\nğŸ“ Inverse Quiz Validation Results (Quiz only, VQA file exists but no valid data):")
            else:
                print("\nğŸ“ Inverse Quiz Validation Results:")
            print("-" * 60)
            print(f"  Question Accuracy : {overall_acc:.2%} ({total_correct}/{total_questions})")
            print(f"  Perfect Image Rate : {perfect_rate:.2%} ({perfect_images}/{len(image_stats)})")
            print("-" * 60)
        else:
            print("âš ï¸  No valid quiz or VQA results found")
    
    # T2I ç»“æœï¼ˆä»… seephysï¼‰
    if "t2i" in metrics and dataset == "seephys":
        t2i_dir = results_dir / "t2i"
        t2i_file = t2i_dir / f"{model}_t2i_metrics.csv"
        
        if t2i_file.exists():
            try:
                df = pd.read_csv(t2i_file)
                valid_df = df.dropna(subset=['psnr', 'ssim', 'clip_score'])
                
                if not valid_df.empty:
                    avg_psnr = valid_df['psnr'].mean()
                    avg_ssim = valid_df['ssim'].mean()
                    avg_clip = valid_df['clip_score'].mean()
                    
                    print("\nğŸ“Š Text-to-Image Metrics:")
                    print("-" * 60)
                    print(f"  PSNR (avg)        : {avg_psnr:.4f}")
                    print(f"  SSIM (avg)        : {avg_ssim:.4f}")
                    print(f"  CLIP Score (avg) : {avg_clip:.4f}")
                    print(f"  Total Samples    : {len(valid_df)}")
                    print("-" * 60)
                    print("  Note: FID score is computed separately and may be in summary_report.txt")
                else:
                    print("âš ï¸  No valid T2I metrics found")
            except Exception as e:
                print(f"âš ï¸  Error reading T2I results: {e}")
        else:
            print("âš ï¸  T2I results file not found")
    
    print()

# ==================== ä¸»å‡½æ•° ====================

def main():
    parser = argparse.ArgumentParser(
        description="SciGenBench Unified Runner - Generate images and evaluate results",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Generate images only
  python run.py --dataset scigen --model gemini-3-pro-imgcoder --mode generate
  
  # Evaluate only
  python run.py --dataset scigen --model gemini-3-pro-imgcoder --mode eval --metric judge
  
  # Generate and evaluate all metrics
  python run.py --dataset scigen --model gemini-3-pro-imgcoder --mode all
  
  # List available models
  python run.py --list-models --dataset scigen
        """
    )
    
    parser.add_argument(
        "--dataset",
        choices=DATASETS,
        required=True,
        help="Dataset to use (scigen or seephys)"
    )
    
    parser.add_argument(
        "--model",
        type=str,
        help="Model name to use for generation/evaluation"
    )
    
    parser.add_argument(
        "--mode",
        choices=["generate", "eval", "all"],
        default="all",
        help="Mode: generate (only), eval (only), or all (generate + eval)"
    )
    
    parser.add_argument(
        "--metric",
        choices=list(EVAL_METRICS.keys()),
        default="all",
        help="Evaluation metric to use (only for eval mode)"
    )
    
    parser.add_argument(
        "--list-models",
        action="store_true",
        help="List all available models for the dataset"
    )
    
    parser.add_argument(
        "-v", "--verbose",
        action="store_true",
        help="Verbose output"
    )
    
    parser.add_argument(
        "--skip-summary",
        action="store_true",
        help="Skip results summary at the end"
    )
    
    args = parser.parse_args()
    
    # åˆ—å‡ºå¯ç”¨æ¨¡å‹
    if args.list_models:
        print(f"\nAvailable models for dataset '{args.dataset}':")
        print("-" * 60)
        for model in AVAILABLE_MODELS[args.dataset]:
            print(f"  - {model}")
        print()
        return
    
    # éªŒè¯æ¨¡å‹
    if not args.model:
        parser.error("--model is required (use --list-models to see available models)")
    
    if args.model not in AVAILABLE_MODELS[args.dataset]:
        print(f"âŒ Error: Model '{args.model}' not available for dataset '{args.dataset}'")
        print(f"\nAvailable models:")
        for model in AVAILABLE_MODELS[args.dataset]:
            print(f"  - {model}")
        sys.exit(1)
    
    # ç¡®å®šè¦è¿è¡Œçš„è¯„ä¼°æŒ‡æ ‡
    if args.mode in ["eval", "all"]:
        if args.metric == "all":
            # æ ¹æ®æ•°æ®é›†é€‰æ‹©å¯ç”¨çš„è¯„ä¼°æŒ‡æ ‡
            if args.dataset == "seephys":
                metrics_to_run = ["judge", "quiz", "t2i"]
            else:  # scigen
                metrics_to_run = ["judge", "quiz"]
        else:
            metrics_to_run = [args.metric]
            # æ£€æŸ¥ t2i æ˜¯å¦ç”¨äºæ­£ç¡®çš„æ•°æ®é›†
            if "t2i" in metrics_to_run and args.dataset != "seephys":
                print(f"âŒ Error: T2I metric is only available for seephys dataset")
                sys.exit(1)
    else:
        metrics_to_run = []
    
    # æ‰§è¡Œä»»åŠ¡
    success = True
    
    # 1. ç”Ÿæˆå›¾åƒ
    gen_success = True
    if args.mode in ["generate", "all"]:
        gen_success = run_generation(args.dataset, args.model, args.verbose)
        if not gen_success:
            print("\nâš ï¸  Generation failed. Continuing with evaluation if requested...")
    
    # 2. è¯„ä¼°ï¼ˆå³ä½¿ç”Ÿæˆå¤±è´¥ï¼Œå¦‚æœç”¨æˆ·æŒ‡å®šäº† eval æ¨¡å¼ï¼Œä¹Ÿè¦è¿è¡Œè¯„ä¼°ï¼‰
    eval_success = True
    if args.mode in ["eval", "all"]:
        for metric in metrics_to_run:
            metric_success = run_evaluation(args.dataset, metric, args.model, args.verbose)
            if not metric_success:
                print(f"\nâš ï¸  Evaluation '{metric}' failed. Continuing...")
                eval_success = False
            
            # å¦‚æœè¿è¡Œäº† quizï¼Œä¸”æ•°æ®é›†æ˜¯ scigenï¼Œè‡ªåŠ¨è¿è¡Œ VQAï¼ˆve_infer.pyï¼‰
            # VQA åªé€‚ç”¨äº scigen æ•°æ®é›†
            if metric == "quiz" and args.dataset == "scigen":
                print(f"\n{'='*60}")
                print(f"ğŸ“Š Running VQA Evaluation (part of quiz, scigen only)")
                print(f"{'='*60}\n")
                vqa_success = run_vqa_evaluation(args.dataset, args.model, args.verbose)
                if not vqa_success:
                    print(f"\nâš ï¸  VQA evaluation failed. Continuing...")
                    eval_success = False
    
    # æ•´ä½“æˆåŠŸçŠ¶æ€ï¼šç”Ÿæˆå’Œè¯„ä¼°éƒ½è¦æˆåŠŸ
    success = gen_success and eval_success
    
    # 3. æ±‡æ€»ç»“æœ
    if not args.skip_summary and metrics_to_run:
        summarize_results(args.dataset, args.model, metrics_to_run)
    
    # é€€å‡ºçŠ¶æ€
    if success:
        print(f"\nâœ… All tasks completed successfully!")
        sys.exit(0)
    else:
        print(f"\nâš ï¸  Some tasks failed. Please check the output above.")
        sys.exit(1)

if __name__ == "__main__":
    main()

